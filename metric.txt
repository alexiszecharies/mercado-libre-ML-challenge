A la hora de elegir la métrica para el modelo en un problema de clasificación lo primero que hay que tener en cuenta es si el target feature es balanceado o desbalanceado. En este caso, al hacer un value counts del target se puede ver que la relación es de aproximadamente 48k a 42k en 90k muestras, por lo que podemos asumir que el problema es balanceado. Por este motivo, accuracy parece una métrica aceptable para el problema.  Sin embargo, no suele ser la métrica que elijo para problemás de clasificación dado que la ROC es una métrica que siempre sirve en los problemás de clasificación y da siempre una pauta de que tan bueno es el modelo. 

Lo bueno que tiene ver el accuracy de modelo es que es algo que la gente de negocio entiende fácilmente usaría esa métricas para hablar de la performance con ellos, dado que es una métricas valida para el problema con la información que se tiene del problema. 

A su vez, debería saber más del negocio para saber si es sensible etiquetar mal used for new o viceversa. En caso de que fuera así, usar la ROC y las métricas asociadas como el recall, etc puede ser importante y permite correr el umbral de probabilidad en la clasifiación teniendo en cuenta las consideraciones de negocio. En ese caso, el accuracy no sería tan buena métrica.